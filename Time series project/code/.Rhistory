x_test <- data[(train_set_size + 1):nrow(data), 1:(lookback - 1), drop = FALSE]
y_test <- data[(train_set_size + 1):nrow(data), lookback, drop = FALSE]
return(list(x_train = x_train, y_train = y_train,
x_test = x_test, y_test = y_test))
}
y_zoo <- xts(y, order.by = seq(from = as.Date("2016-01-01"), to = as.Date("2021-12-31"), by = 1), unique = TRUE, tzone = "UTC")
#set apple data as dataframe
data<- data.frame(
date = seq(from = as.Date("2016-01-01"), to = as.Date("2021-12-31"), by = 1),
AQ_nh3 = as.numeric(y_zoo))
head(data)
# Scaling the response variable
AQ_nh3 <- scale(data$AQ_nh3)
AQ_nh3 <- data.frame(AQ_nh3)
head(AQ_nh3)
#divide data into train and test 80% - 20%
lookback <- 8 # choose sequence length
split_data <- split_data(AQ_nh3, lookback) # assuming "price" is a data frame
x_train <- split_data$x_train
y_train <- split_data$y_train
x_test <- split_data$x_test
y_test <- split_data$y_test
cat(paste('x_train.shape = ', dim(x_train), '\n'))
cat(paste('y_train.shape = ', dim(y_train), '\n'))
cat(paste('x_test.shape = ', dim(x_test), '\n'))
cat(paste('y_test.shape = ', dim(y_test), '\n'))
#decide hyperparameters
input_dim <- 1
hidden_dim <- 32
num_layers <- 3
output_dim <- 1
num_epochs <- 50
# Reshape the training and test data to have a 3D tensor shape
x_train <- array_reshape(x_train, c(dim(x_train)[1], lookback-1, input_dim))
x_test <- array_reshape(x_test, c(dim(x_test)[1], lookback-1, input_dim))
# Define the LSTM model using Keras
ltsm_model <- keras_model_sequential() %>%
layer_lstm(units = hidden_dim, return_sequences = TRUE, input_shape = c(lookback-1, input_dim)) %>%
layer_lstm(units = hidden_dim) %>%
layer_dense(units = output_dim)
# Compile the model using the mean squared error loss and the Adam optimizer
ltsm_model %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(learning_rate = 0.01))
# Train the model on the training data
history <- ltsm_model %>% fit(x_train, y_train, epochs = num_epochs, batch_size = 16, validation_data = list(x_test, y_test))
# Extract predictions from the stimated model
y_train_pred <- ltsm_model %>% predict(x_train)
y_test_pred <- ltsm_model %>% predict(x_test)
# Rescale the predictions and original values
y_train_pred_orig <- y_train_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_train_orig <- y_train * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_pred_orig <- y_test_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_orig <- y_test * sd(data$AQ_nh3) + mean(data$AQ_nh3)
# plot the predictions from traning data and train Loss
# Set up the layout of the plots
par(mfrow = c(1, 2))
options(repr.plot.width=15, repr.plot.height=5)
# Plot the training and predicted values
plot(y_train_orig, type = "l",main="Daily AQ_nh3 values", col = "green", xlab = "Days", ylab = "AQ_nh3", lwd=3)
#lines(y_train_orig, col = "green")
lines(y_train_pred_orig, col = "red")
legend(x = "topleft", legend = c("Train", "Train Predictions"), col = c("green", "red"), lwd = 2)
grid()
# Plot the loss of training data
plot(history$metrics$loss, type = "l",main="Traning Loss", xlab = "Epochs", ylab = "Loss", col = "blue",lwd=3)
grid()
# Shift the predicted values to start from where the training data predictions end
shift <- length(y_train_pred_orig)
y_test_pred_orig_shifted <- c(rep(NA, shift), y_test_pred_orig[,1])
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
grid()
mse <- history$metrics$val_loss[length(history$metrics$val_loss)]
mse <- round(mse, 7)
mse
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
rmse_ltsm_validation = sqrt(mean(res_ltsm^2))
par(mfrow = c(1,5))
hist(res_ltsm,40,
xlab = "Value",
main = "Empirical distribution of residuals")
plot(res_ltsm, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_ltsm),digits = 4),
"- var:", round(var(res_ltsm),digits = 4)))
abline(c(0,0),c(0,length(res_ltsm)), col= "red", lwd = 2)
boxplot(res_ltsm, ylab = "Residuals", main = "Outliers")$out
qqnorm(res_ltsm, main='Residuals')
qqline(res_ltsm)
plot(scale(y_validation), res_ltsm, xlab = "y_validation", ylab = "Residuals LTSM", main = "Scatter plot of residuals")
# training statistics
ltsm_metrics <- calculate_metrics(ltsm_model, y_train_pred_orig, y_train_orig, y_test_pred_orig, y_test_orig)
print(ltsm_metrics)
shapiro.test(res_ltsm)                             # H0: normally distributed
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
Box.test(res_ltsm, lag = 7, type = "Ljung-Box")    # H0: no correlation
Box.test(res_ltsm, lag = 30, type = "Ljung-Box")
Box.test(res_ltsm, lag = 365, type = "Ljung-Box")
#########################################################
par(mfrow = c(1,3))
plot(scale(y_validation), res_xgb, xlab = "y_validation", ylab = "Residuals XGBoost", main = "Scatter plot of residuals")
plot(scale(y_validation), res_prophet, xlab = "y_validation", ylab = "Residuals Prophet", main = "Scatter plot of residuals")
# plot the predictions from traning data and train Loss
# Set up the layout of the plots
par(mfrow = c(1, 2))
options(repr.plot.width=15, repr.plot.height=5)
# Plot the training and predicted values
plot(y_train_orig, type = "l",main="Daily AQ_nh3 values", col = "green", xlab = "Days", ylab = "AQ_nh3", lwd=3)
#lines(y_train_orig, col = "green")
lines(y_train_pred_orig, col = "red")
legend(x = "topleft", legend = c("Train", "Train Predictions"), col = c("green", "red"), lwd = 2)
grid()
# Plot the loss of training data
plot(history$metrics$loss, type = "l",main="Traning Loss", xlab = "Epochs", ylab = "Loss", col = "blue",lwd=3)
grid()
# Shift the predicted values to start from where the training data predictions end
shift <- length(y_train_pred_orig)
y_test_pred_orig_shifted <- c(rep(NA, shift), y_test_pred_orig[,1])
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
grid()
mse <- history$metrics$val_loss[length(history$metrics$val_loss)]
mse <- round(mse, 7)
mse
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
rmse_ltsm_validation = sqrt(mean(res_ltsm^2))
par(mfrow = c(1,5))
hist(res_ltsm,40,
xlab = "Value",
main = "Empirical distribution of residuals")
plot(res_ltsm, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_ltsm),digits = 4),
"- var:", round(var(res_ltsm),digits = 4)))
abline(c(0,0),c(0,length(res_ltsm)), col= "red", lwd = 2)
boxplot(res_ltsm, ylab = "Residuals", main = "Outliers")$out
qqnorm(res_ltsm, main='Residuals')
qqline(res_ltsm)
plot(scale(y_validation), res_ltsm, xlab = "y_validation", ylab = "Residuals LTSM", main = "Scatter plot of residuals")
# training statistics
ltsm_metrics <- calculate_metrics(ltsm_model, y_train_pred_orig, y_train_orig, y_test_pred_orig, y_test_orig)
print(ltsm_metrics)
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
shapiro.test(res_ltsm)                             # H0: normally distributed
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
shapiro.test(res_ltsm)                             # H0: normally distributed
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
Box.test(res_ltsm, lag = 7, type = "Ljung-Box")    # H0: no correlation
Box.test(res_ltsm, lag = 30, type = "Ljung-Box")
Box.test(res_ltsm, lag = 365, type = "Ljung-Box")
library(quantmod)
library(keras)
#  define the train and test split
split_data <- function(stock, lookback) {
data_raw <- as.matrix(stock) # convert to matrix
data <- array(dim = c(0, lookback, ncol(data_raw)))
# create all possible sequences of length lookback
for (index in 1:(nrow(data_raw) - lookback)) {
data <- rbind(data, data_raw[index:(index + lookback - 1), ])
}
test_set_size <- round(0.2 * nrow(data)) + 1
train_set_size <- nrow(data) - test_set_size
x_train <- data[1:train_set_size, 1:(lookback - 1), drop = FALSE]
y_train <- data[1:train_set_size, lookback, drop = FALSE]
x_test <- data[(train_set_size + 1):nrow(data), 1:(lookback - 1), drop = FALSE]
y_test <- data[(train_set_size + 1):nrow(data), lookback, drop = FALSE]
return(list(x_train = x_train, y_train = y_train,
x_test = x_test, y_test = y_test))
}
y_zoo <- xts(y, order.by = seq(from = as.Date("2016-01-01"), to = as.Date("2021-12-31"), by = 1), unique = TRUE, tzone = "UTC")
#set apple data as dataframe
data<- data.frame(
date = seq(from = as.Date("2016-01-01"), to = as.Date("2021-12-31"), by = 1),
AQ_nh3 = as.numeric(y_zoo))
head(data)
# Scaling the response variable
AQ_nh3 <- scale(data$AQ_nh3)
AQ_nh3 <- data.frame(AQ_nh3)
head(AQ_nh3)
#divide data into train and test 80% - 20%
lookback <- 8 # choose sequence length
split_data <- split_data(AQ_nh3, lookback) # assuming "price" is a data frame
x_train <- split_data$x_train
y_train <- split_data$y_train
x_test <- split_data$x_test
y_test <- split_data$y_test
cat(paste('x_train.shape = ', dim(x_train), '\n'))
cat(paste('y_train.shape = ', dim(y_train), '\n'))
cat(paste('x_test.shape = ', dim(x_test), '\n'))
cat(paste('y_test.shape = ', dim(y_test), '\n'))
#decide hyperparameters
input_dim <- 1
hidden_dim <- 32
num_layers <- 3
output_dim <- 1
num_epochs <- 100
# Reshape the training and test data to have a 3D tensor shape
x_train <- array_reshape(x_train, c(dim(x_train)[1], lookback-1, input_dim))
x_test <- array_reshape(x_test, c(dim(x_test)[1], lookback-1, input_dim))
# Define the LSTM model using Keras
ltsm_model <- keras_model_sequential() %>%
layer_lstm(units = hidden_dim, return_sequences = TRUE, input_shape = c(lookback-1, input_dim)) %>%
layer_lstm(units = hidden_dim) %>%
layer_dense(units = output_dim)
# Compile the model using the mean squared error loss and the Adam optimizer
ltsm_model %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(learning_rate = 0.01))
# Train the model on the training data
history <- ltsm_model %>% fit(x_train, y_train, epochs = num_epochs, batch_size = 16, validation_data = list(x_test, y_test))
# Extract predictions from the stimated model
y_train_pred <- ltsm_model %>% predict(x_train)
y_test_pred <- ltsm_model %>% predict(x_test)
# Rescale the predictions and original values
y_train_pred_orig <- y_train_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_train_orig <- y_train * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_pred_orig <- y_test_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_orig <- y_test * sd(data$AQ_nh3) + mean(data$AQ_nh3)
# plot the predictions from traning data and train Loss
# Set up the layout of the plots
par(mfrow = c(1, 2))
options(repr.plot.width=15, repr.plot.height=5)
# Plot the training and predicted values
plot(y_train_orig, type = "l",main="Daily AQ_nh3 values", col = "green", xlab = "Days", ylab = "AQ_nh3", lwd=3)
#lines(y_train_orig, col = "green")
lines(y_train_pred_orig, col = "red")
legend(x = "topleft", legend = c("Train", "Train Predictions"), col = c("green", "red"), lwd = 2)
grid()
# Plot the loss of training data
plot(history$metrics$loss, type = "l",main="Traning Loss", xlab = "Epochs", ylab = "Loss", col = "blue",lwd=3)
grid()
# Shift the predicted values to start from where the training data predictions end
shift <- length(y_train_pred_orig)
y_test_pred_orig_shifted <- c(rep(NA, shift), y_test_pred_orig[,1])
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
grid()
mse <- history$metrics$val_loss[length(history$metrics$val_loss)]
mse <- round(mse, 7)
mse
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
rmse_ltsm_validation = sqrt(mean(res_ltsm^2))
par(mfrow = c(1,5))
hist(res_ltsm,40,
xlab = "Value",
main = "Empirical distribution of residuals")
plot(res_ltsm, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_ltsm),digits = 4),
"- var:", round(var(res_ltsm),digits = 4)))
abline(c(0,0),c(0,length(res_ltsm)), col= "red", lwd = 2)
boxplot(res_ltsm, ylab = "Residuals", main = "Outliers")$out
qqnorm(res_ltsm, main='Residuals')
qqline(res_ltsm)
plot(scale(y_validation), res_ltsm, xlab = "y_validation", ylab = "Residuals LTSM", main = "Scatter plot of residuals")
# training statistics
ltsm_metrics <- calculate_metrics(ltsm_model, y_train_pred_orig, y_train_orig, y_test_pred_orig, y_test_orig)
print(ltsm_metrics)
shapiro.test(res_ltsm)                             # H0: normally distributed
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
Box.test(res_ltsm, lag = 7, type = "Ljung-Box")    # H0: no correlation
Box.test(res_ltsm, lag = 30, type = "Ljung-Box")
Box.test(res_ltsm, lag = 365, type = "Ljung-Box")
#########################################################
par(mfrow = c(1,3))
plot(scale(y_validation), res_xgb, xlab = "y_validation", ylab = "Residuals XGBoost", main = "Scatter plot of residuals")
plot(scale(y_validation), res_prophet, xlab = "y_validation", ylab = "Residuals Prophet", main = "Scatter plot of residuals")
# training statistics
ltsm_metrics <- calculate_metrics(ltsm_model, y_train_pred_orig, y_train_orig, y_test_pred_orig, y_test_orig)
# Extract predictions from the stimated model
y_train_pred <- ltsm_model %>% predict(x_train)
y_test_pred <- ltsm_model %>% predict(x_test)
# Rescale the predictions and original values
y_train_pred_orig <- y_train_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_train_orig <- y_train * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_pred_orig <- y_test_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_orig <- y_test * sd(data$AQ_nh3) + mean(data$AQ_nh3)
# plot the predictions from traning data and train Loss
# Set up the layout of the plots
par(mfrow = c(1, 2))
options(repr.plot.width=15, repr.plot.height=5)
# Plot the training and predicted values
plot(y_train_orig, type = "l",main="Daily AQ_nh3 values", col = "green", xlab = "Days", ylab = "AQ_nh3", lwd=3)
#lines(y_train_orig, col = "green")
lines(y_train_pred_orig, col = "red")
legend(x = "topleft", legend = c("Train", "Train Predictions"), col = c("green", "red"), lwd = 2)
grid()
# Plot the loss of training data
plot(history$metrics$loss, type = "l",main="Traning Loss", xlab = "Epochs", ylab = "Loss", col = "blue",lwd=3)
grid()
# Shift the predicted values to start from where the training data predictions end
shift <- length(y_train_pred_orig)
y_test_pred_orig_shifted <- c(rep(NA, shift), y_test_pred_orig[,1])
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
grid()
mse <- history$metrics$val_loss[length(history$metrics$val_loss)]
mse <- round(mse, 7)
mse
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
rmse_ltsm_validation = sqrt(mean(res_ltsm^2))
par(mfrow = c(1,5))
hist(res_ltsm,40,
xlab = "Value",
main = "Empirical distribution of residuals")
plot(res_ltsm, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_ltsm),digits = 4),
"- var:", round(var(res_ltsm),digits = 4)))
abline(c(0,0),c(0,length(res_ltsm)), col= "red", lwd = 2)
boxplot(res_ltsm, ylab = "Residuals", main = "Outliers")$out
qqnorm(res_ltsm, main='Residuals')
qqline(res_ltsm)
plot(scale(y_validation), res_ltsm, xlab = "y_validation", ylab = "Residuals LTSM", main = "Scatter plot of residuals")
# training statistics
ltsm_metrics <- calculate_metrics(ltsm_model, y_train_pred_orig, y_train_orig, y_test_pred_orig, y_test_orig)
print(ltsm_metrics)
shapiro.test(res_ltsm)                             # H0: normally distributed
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
Box.test(res_ltsm, lag = 7, type = "Ljung-Box")    # H0: no correlation
shapiro.test(res_ltsm)                             # H0: normally distributed
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
Box.test(res_ltsm, lag = 7, type = "Ljung-Box")    # H0: no correlation
Box.test(res_ltsm, lag = 30, type = "Ljung-Box")
Box.test(res_ltsm, lag = 365, type = "Ljung-Box")
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
shapiro.test(res_ltsm)                             # H0: normally distributed
if(!require('quantmod')) install.packages('quantmod')
if(!require('keras')) install.packages('keras')
library(quantmod)
library(keras)
#  define the train and test split
split_data <- function(stock, lookback) {
data_raw <- as.matrix(stock) # convert to matrix
data <- array(dim = c(0, lookback, ncol(data_raw)))
# create all possible sequences of length lookback
for (index in 1:(nrow(data_raw) - lookback)) {
data <- rbind(data, data_raw[index:(index + lookback - 1), ])
}
test_set_size <- round(0.2 * nrow(data)) + 1
train_set_size <- nrow(data) - test_set_size
x_train <- data[1:train_set_size, 1:(lookback - 1), drop = FALSE]
y_train <- data[1:train_set_size, lookback, drop = FALSE]
x_test <- data[(train_set_size + 1):nrow(data), 1:(lookback - 1), drop = FALSE]
y_test <- data[(train_set_size + 1):nrow(data), lookback, drop = FALSE]
return(list(x_train = x_train, y_train = y_train,
x_test = x_test, y_test = y_test))
}
y_zoo <- xts(y, order.by = seq(from = as.Date("2016-01-01"), to = as.Date("2021-12-31"), by = 1), unique = TRUE, tzone = "UTC")
#set apple data as dataframe
data<- data.frame(
date = seq(from = as.Date("2016-01-01"), to = as.Date("2021-12-31"), by = 1),
AQ_nh3 = as.numeric(y_zoo))
head(data)
# Scaling the response variable
AQ_nh3 <- scale(data$AQ_nh3)
AQ_nh3 <- data.frame(AQ_nh3)
head(AQ_nh3)
#divide data into train and test 80% - 20%
lookback <- 8 # choose sequence length
split_data <- split_data(AQ_nh3, lookback) # assuming "price" is a data frame
x_train <- split_data$x_train
y_train <- split_data$y_train
x_test <- split_data$x_test
y_test <- split_data$y_test
cat(paste('x_train.shape = ', dim(x_train), '\n'))
cat(paste('y_train.shape = ', dim(y_train), '\n'))
cat(paste('x_test.shape = ', dim(x_test), '\n'))
cat(paste('y_test.shape = ', dim(y_test), '\n'))
#decide hyperparameters
input_dim <- 1
hidden_dim <- 32
num_layers <- 3
output_dim <- 1
num_epochs <- 100
# Reshape the training and test data to have a 3D tensor shape
x_train <- array_reshape(x_train, c(dim(x_train)[1], lookback-1, input_dim))
x_test <- array_reshape(x_test, c(dim(x_test)[1], lookback-1, input_dim))
# Define the LSTM model using Keras
ltsm_model <- keras_model_sequential() %>%
layer_lstm(units = hidden_dim, return_sequences = TRUE, input_shape = c(lookback-1, input_dim)) %>%
layer_lstm(units = hidden_dim) %>%
layer_dense(units = output_dim)
# Compile the model using the mean squared error loss and the Adam optimizer
ltsm_model %>% compile(loss = "mean_squared_error", optimizer = optimizer_adam(learning_rate = 0.01))
# Train the model on the training data
history <- ltsm_model %>% fit(x_train, y_train, epochs = num_epochs, batch_size = 16, validation_data = list(x_test, y_test))
# Extract predictions from the stimated model
y_train_pred <- ltsm_model %>% predict(x_train)
y_test_pred <- ltsm_model %>% predict(x_test)
# Rescale the predictions and original values
y_train_pred_orig <- y_train_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_train_orig <- y_train * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_pred_orig <- y_test_pred * sd(data$AQ_nh3) + mean(data$AQ_nh3)
y_test_orig <- y_test * sd(data$AQ_nh3) + mean(data$AQ_nh3)
# plot the predictions from traning data and train Loss
# Set up the layout of the plots
par(mfrow = c(1, 2))
options(repr.plot.width=15, repr.plot.height=5)
# Plot the training and predicted values
plot(y_train_orig, type = "l",main="Daily AQ_nh3 values", col = "green", xlab = "Days", ylab = "AQ_nh3", lwd=3)
#lines(y_train_orig, col = "green")
lines(y_train_pred_orig, col = "red")
legend(x = "topleft", legend = c("Train", "Train Predictions"), col = c("green", "red"), lwd = 2)
grid()
# Plot the loss of training data
plot(history$metrics$loss, type = "l",main="Traning Loss", xlab = "Epochs", ylab = "Loss", col = "blue",lwd=3)
grid()
# Shift the predicted values to start from where the training data predictions end
shift <- length(y_train_pred_orig)
y_test_pred_orig_shifted <- c(rep(NA, shift), y_test_pred_orig[,1])
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
grid()
mse <- history$metrics$val_loss[length(history$metrics$val_loss)]
mse <- round(mse, 7)
mse
# Residual distribution
res_ltsm = y_test_orig - y_test_pred_orig
rmse_ltsm_validation = sqrt(mean(res_ltsm^2))
par(mfrow = c(1,5))
hist(res_ltsm,40,
xlab = "Value",
main = "Empirical distribution of residuals")
plot(res_ltsm, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_ltsm),digits = 4),
"- var:", round(var(res_ltsm),digits = 4)))
abline(c(0,0),c(0,length(res_ltsm)), col= "red", lwd = 2)
boxplot(res_ltsm, ylab = "Residuals", main = "Outliers")$out
qqnorm(res_ltsm, main='Residuals')
qqline(res_ltsm)
plot(scale(y_validation), res_ltsm, xlab = "y_validation", ylab = "Residuals LTSM", main = "Scatter plot of residuals")
# training statistics
ltsm_metrics <- calculate_metrics(ltsm_model, y_train_pred_orig, y_train_orig, y_test_pred_orig, y_test_orig)
print(ltsm_metrics)
shapiro.test(res_ltsm)                             # H0: normally distributed
bptest(lm(res_ltsm ~ y_validation))                # H0: omoschedasticity
Box.test(res_ltsm, lag = 7, type = "Ljung-Box")    # H0: no correlation
Box.test(res_ltsm, lag = 30, type = "Ljung-Box")
Box.test(res_ltsm, lag = 365, type = "Ljung-Box")
#########################################################
par(mfrow = c(1,3))
plot(scale(y_validation), res_xgb, xlab = "y_validation", ylab = "Residuals XGBoost", main = "Scatter plot of residuals")
plot(scale(y_validation), res_prophet, xlab = "y_validation", ylab = "Residuals Prophet", main = "Scatter plot of residuals")
# plot the predictions from traning data and train Loss
# Set up the layout of the plots
par(mfrow = c(1, 2))
options(repr.plot.width=15, repr.plot.height=5)
# Plot the training and predicted values
plot(y_train_orig, type = "l",main="Daily AQ_nh3 values", col = "green", xlab = "Days", ylab = "AQ_nh3", lwd=3)
#lines(y_train_orig, col = "green")
lines(y_train_pred_orig, col = "red")
legend(x = "topleft", legend = c("Train", "Train Predictions"), col = c("green", "red"), lwd = 2)
grid()
# Plot the loss of training data
plot(history$metrics$loss, type = "l",main="Traning Loss", xlab = "Epochs", ylab = "Loss", col = "blue",lwd=3)
grid()
# Shift the predicted values to start from where the training data predictions end
shift <- length(y_train_pred_orig)
y_test_pred_orig_shifted <- c(rep(NA, shift), y_test_pred_orig[,1])
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
grid()
# Plot the training and predicted values
par(mfrow = c(1, 1))
options(repr.plot.width=12, repr.plot.height=8)
plot(data$AQ_nh3, type = "l", main="LSTM AQ_nh3 predictions",col = "green", xlab = "Days", ylab = "AQ_nh3",lwd=3)
lines(y_train_pred_orig, col = "blue",lwd=3)
lines(y_test_pred_orig_shifted, col = "red",lwd=3)
legend(x = "topleft", legend = c("Original", "Train Predictions","Test-Prediction"), col = c("green","blue" ,"red"), lwd = 2)
par(mfrow = c(1,5))
hist(res_ltsm,40,
xlab = "Value",
main = "Empirical distribution of residuals")
plot(res_ltsm, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_ltsm),digits = 4),
"- var:", round(var(res_ltsm),digits = 4)))
abline(c(0,0),c(0,length(res_ltsm)), col= "red", lwd = 2)
boxplot(res_ltsm, ylab = "Residuals", main = "Outliers")$out
qqnorm(res_ltsm, main='Residuals')
qqline(res_ltsm)
plot(scale(y_validation), res_ltsm, xlab = "y_validation", ylab = "Residuals LTSM", main = "Scatter plot of residuals")
